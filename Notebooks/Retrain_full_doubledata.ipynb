{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.misc\n",
    "image = tf.keras.preprocessing.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_classes = 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/maxim/notebooks/ObjectRecognition/datasets'\n",
    "\n",
    "def get_path(path):\n",
    "    return os.path.join(base_path, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat(get_path('devkit/cars_meta.mat'))\n",
    "label_names = np.array([row[0] for row in mat['class_names'].reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat(get_path('devkit/cars_annos.mat'))\n",
    "file_labels = {row[0][0]: row[5].squeeze() - 1 for row in mat['annotations'][0]}\n",
    "\n",
    "filenames_np = np.array([get_path(row[0][0]) for row in mat['annotations'][0]])\n",
    "\n",
    "labels_np = np.array([row[5].squeeze() - 1 for row in mat['annotations'][0]])\n",
    "labels_one_hot = np.zeros((labels_np.size, output_layer_classes))\n",
    "labels_one_hot[np.arange(labels_np.size), labels_np] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_train_np, filenames_test_np, labels_train_np, labels_test_np = \\\n",
    "    train_test_split(filenames_np, labels_one_hot, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_train = tf.constant(filenames_train_np)\n",
    "labels_train = tf.constant(labels_train_np)\n",
    "filenames_test = tf.constant(filenames_test_np)\n",
    "labels_test = tf.constant(labels_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((filenames_train, labels_train))\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((filenames_test, labels_test))\n",
    "\n",
    "image_count = int(filenames_train.shape[0])\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 64\n",
    "STEPS_PER_EPOCH = np.ceil(2 * image_count / BATCH_SIZE)\n",
    "\n",
    "\n",
    "def convert(filename, label):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_jpeg(img, channels=CHANNELS)\n",
    "     # img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = (img/127.5) - 1\n",
    "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "    return img, label\n",
    "\n",
    "def augment(filename, label):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.image.decode_jpeg(img, channels=CHANNELS)\n",
    "     # img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = (img / 127.5) - 1\n",
    "    img = tf.image.resize(img, [2 * IMG_WIDTH, 2 * IMG_HEIGHT])\n",
    "    \n",
    "    img = tf.image.random_crop(img, size=[IMG_WIDTH, IMG_HEIGHT, CHANNELS]) # Random crop back to 28x28\n",
    "    img = tf.image.random_brightness(img, max_delta=0.5) # Random brightness\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "\n",
    "    return img,label\n",
    "\n",
    "database_train1 = dataset_train.map(convert)\n",
    "database_train2 = dataset_train.map(augment)\n",
    "database_train = database_train1.concatenate(database_train2)\n",
    "\n",
    "database_test = dataset_test.map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "def show_batch(image_batch, label_batch):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        plt.title(label_names[label_batch[n]])\n",
    "        plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = prepare_for_training(database_train, cache='cache')\n",
    "test_ds = prepare_for_training(database_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, CHANNELS)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  155\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "# Fine-tune from this layer onwards\n",
    "# fine_tune_at = 100\n",
    "\n",
    "# # Freeze all the layers before the `fine_tune_at` layer\n",
    "# for layer in base_model.layers[:fine_tune_at]:\n",
    "#     layer.trainable =  False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(output_layer_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "validation_steps=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 481.0 steps, validate for 20 steps\n",
      "Epoch 1/10\n",
      "481/481 [==============================] - 128s 266ms/step - loss: 3.8474 - accuracy: 0.2024 - val_loss: 3.6380 - val_accuracy: 0.1469\n",
      "Epoch 2/10\n",
      "481/481 [==============================] - 117s 242ms/step - loss: 1.7243 - accuracy: 0.6172 - val_loss: 2.4835 - val_accuracy: 0.3875\n",
      "Epoch 3/10\n",
      "481/481 [==============================] - 117s 242ms/step - loss: 0.9591 - accuracy: 0.7921 - val_loss: 1.8095 - val_accuracy: 0.5750\n",
      "Epoch 4/10\n",
      "481/481 [==============================] - 117s 243ms/step - loss: 0.5585 - accuracy: 0.8947 - val_loss: 1.4251 - val_accuracy: 0.6492\n",
      "Epoch 5/10\n",
      "481/481 [==============================] - 117s 243ms/step - loss: 0.3224 - accuracy: 0.9517 - val_loss: 1.1070 - val_accuracy: 0.7242\n",
      "Epoch 6/10\n",
      "481/481 [==============================] - 117s 244ms/step - loss: 0.1849 - accuracy: 0.9799 - val_loss: 0.9656 - val_accuracy: 0.7422\n",
      "Epoch 7/10\n",
      "481/481 [==============================] - 117s 243ms/step - loss: 0.1055 - accuracy: 0.9930 - val_loss: 0.7854 - val_accuracy: 0.7711\n",
      "Epoch 8/10\n",
      "481/481 [==============================] - 117s 243ms/step - loss: 0.0651 - accuracy: 0.9962 - val_loss: 0.7306 - val_accuracy: 0.7992\n",
      "Epoch 9/10\n",
      "481/481 [==============================] - 117s 243ms/step - loss: 0.0449 - accuracy: 0.9972 - val_loss: 0.6889 - val_accuracy: 0.8047\n",
      "Epoch 10/10\n",
      "481/481 [==============================] - 117s 244ms/step - loss: 0.0335 - accuracy: 0.9976 - val_loss: 0.6897 - val_accuracy: 0.8055\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds,\n",
    "                    epochs=initial_epochs,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    validation_data=test_ds,\n",
    "                    validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 55ms/step - loss: 0.6690 - accuracy: 0.8156\n",
      "loss: 0.67\n",
      "accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "loss0,accuracy0 = model.evaluate(test_ds, steps=validation_steps)\n",
    "print(\"loss: {:.2f}\".format(loss0))\n",
    "print(\"accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cars_full_aug10_double_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 481.0 steps, validate for 20 steps\n",
      "481/481 [==============================] - 117s 243ms/step - loss: 0.0276 - accuracy: 0.9975 - val_loss: 0.6349 - val_accuracy: 0.8211\n"
     ]
    }
   ],
   "source": [
    "history1 = model.fit(train_ds,\n",
    "                     epochs=1,\n",
    "                     steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                     validation_data=test_ds,\n",
    "                     validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 60ms/step - loss: 0.6629 - accuracy: 0.8156\n",
      "loss: 0.66\n",
      "accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "loss0,accuracy0 = model.evaluate(test_ds, steps=validation_steps)\n",
    "print(\"loss: {:.2f}\".format(loss0))\n",
    "print(\"accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 481.0 steps, validate for 20 steps\n",
      "481/481 [==============================] - 117s 243ms/step - loss: 0.0239 - accuracy: 0.9976 - val_loss: 0.6529 - val_accuracy: 0.8281\n"
     ]
    }
   ],
   "source": [
    "history1 = model.fit(train_ds,\n",
    "                     epochs=1,\n",
    "                     steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                     validation_data=test_ds,\n",
    "                     validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 61ms/step - loss: 0.6380 - accuracy: 0.8320\n",
      "loss: 0.64\n",
      "accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "loss0,accuracy0 = model.evaluate(test_ds, steps=validation_steps)\n",
    "print(\"loss: {:.2f}\".format(loss0))\n",
    "print(\"accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cars_full_aug12_double_data.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
